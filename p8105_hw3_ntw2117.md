P8105: Homework 3
================
Nick Williams

Problem 1
---------

#### Importing and cleaning BRFSS data

``` r
brfss_data <- p8105.datasets::brfss_smart2010 %>% 
  janitor::clean_names()

brfss_data <- brfss_data %>% 
  rename(state = locationabbr, 
         location = locationdesc) %>% 
  filter(topic == "Overall Health") %>% 
  select(-(class:question), 
                       -sample_size, 
                       -(confidence_limit_low:geo_location)) %>% 
  mutate(response = as.factor(response))

brfss_data$response <- fct_relevel(brfss_data$response, 
                                   "Excellent", 
                                   "Very good", 
                                   "Good")
```

#### Questions and plots

``` r
brfss_data %>% 
  filter(year == 2002) %>% 
  distinct(state, location) %>% 
  count(state) %>% 
  filter(n == 7) %>% 
  rename("State" = state, 
         "# Locations" = n) %>% 
  knitr::kable()
```

| State |  \# Locations|
|:------|-------------:|
| CT    |             7|
| FL    |             7|
| NC    |             7|

In 2002, the states Connecticut, Florida, and North Carolina were observed at 7 locations.

``` r
brfss_data %>% 
  group_by(state, year) %>% 
  distinct(location) %>% 
  count(state) %>% 
  ggplot(aes(x = year, y = n)) + 
    geom_line(aes(color = state)) + 
    viridis::scale_color_viridis(
      name = "", 
      discrete = TRUE, 
      option = "magma") +
    labs(title = "Figure 1: Spaghetti plot of number of locations across states from 2002 to 2010", 
         x = "Year", 
         y = "Number of Locations") + 
    theme(legend.position = "bottom", 
          legend.direction = "horizontal", 
          legend.key.width = unit(.1, "in")) + 
    guides(color = guide_legend(ncol = 17))
```

<img src="p8105_hw3_ntw2117_files/figure-markdown_github/creating spaghetti plot-1.png" width="90%" style="display: block; margin: auto;" />

Figure 1 shows the trends in the number of locations across all 50 states and the District of Columbia. From 2002 to 2010, all states except Florida had less than 20 locations. Furthermore, it appears that most states had a relatively constant number of locations over the eight year period.

``` r
brfss_data %>% 
  filter(year %in% c(2002, 2006, 2010), 
         state == "NY") %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>% 
  mutate(prop_excel =  (excellent) / (excellent + fair + good + poor + very_good)) %>% 
  group_by(year) %>% 
  summarize("Average proportion excellent" = round(mean(prop_excel), 4), 
            "Standard deviation" = round(sd(prop_excel), 4)) %>% 
  rename(Year = year) %>% 
  knitr::kable()
```

|  Year|  Average proportion excellent|  Standard deviation|
|-----:|-----------------------------:|-------------------:|
|  2002|                        0.2405|              0.0450|
|  2006|                        0.2253|              0.0400|
|  2010|                        0.2270|              0.0356|

The above table contains the average proportion of excellent responses across locations in New York for 2002, 2006, and 2010. A small decrease is observed from 2002 to 2006 and then remains constant from 2006 to 2010. The standard error decreases in 2010, most likely indicating that more responses were collected that year.

``` r
library(patchwork)

avg_response_year_state <- 
  brfss_data %>%
  mutate(year = as.factor(year)) %>% 
  group_by(year, state, response) %>%
  summarize(avg_response = mean(data_value))

poor_dist <- avg_response_year_state %>%
  filter(response == "Poor") %>%
  ggplot(aes(x = avg_response, fill = year)) +
    geom_density(alpha = 0.5, color = NA) + 
    geom_text(x = 0, y = 0.40, 
              label = "Poor", 
              size = 3) + 
    scale_x_continuous(limits = c(0, 45), 
                       breaks = c(0, 5, 10, 15, 20, 25, 30, 35, 40, 45)) + 
    viridis::scale_fill_viridis(discrete = TRUE, 
                                option = "magma",
                                guide = FALSE) + 
    theme(axis.text.x = element_blank(), 
          axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank())

fair_dist <- avg_response_year_state %>%
  filter(response == "Fair") %>%
  ggplot(aes(x = avg_response, fill = year)) +
    geom_density(alpha = 0.5, color = NA) +
    geom_text(x = 0, y = 0.15, 
              label = "Fair", size = 3) + 
    scale_x_continuous(limits = c(0, 45), 
                       breaks = c(0, 5, 10, 15, 20, 25, 30, 35, 40, 45)) + 
    viridis::scale_fill_viridis(discrete = TRUE, 
                                option = "magma",
                                guide = FALSE) + 
    theme(axis.text.x = element_blank(), 
          axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank())

good_dist <- avg_response_year_state %>%
  filter(response == "Good") %>%
  ggplot(aes(x = avg_response, fill = year)) +
    geom_density(alpha = 0.5, color = NA) +
    geom_text(x = 0, y = 0.15, 
              label = "Good", size = 3) + 
    scale_x_continuous(limits = c(0, 45), 
                       breaks = c(0, 5, 10, 15, 20, 25, 30, 35, 40, 45)) + 
    viridis::scale_fill_viridis(discrete = TRUE, 
                                option = "magma", 
                                guide = FALSE) + 
    theme(axis.text.x = element_blank(),
          axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank()) 

v_good_dist <- avg_response_year_state %>%
  filter(response == "Very good") %>%
  ggplot(aes(x = avg_response, fill = year)) +
    geom_density(alpha = 0.5, color = NA) +
    geom_text(x = 1, y = 0.125, 
              label = "Very good", size = 3) + 
    scale_x_continuous(limits = c(0, 45), 
                       breaks = c(0, 5, 10, 15, 20, 25, 30, 35, 40, 45)) + 
    viridis::scale_fill_viridis(discrete = TRUE, 
                                option = "magma",
                                guide = FALSE) + 
    theme(axis.text.x = element_blank(), 
          axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank())

excel_dist <- avg_response_year_state %>%
  filter(response == "Excellent") %>%
  ggplot(aes(x = avg_response, fill = year)) +
    geom_density(alpha = 0.5, color = NA) +
    geom_text(x = 1, y = 0.15, 
              label = "Excellent", size = 3) + 
    scale_x_continuous(limits = c(0, 45), 
                       breaks = c(0, 5, 10, 15, 20, 25, 30, 35, 40, 45)) + 
    labs(y = "", x = "Response proportion") +
    viridis::scale_fill_viridis(name = "Year",
                                discrete = TRUE, 
                                option = "magma") +  
    theme(legend.position = "bottom",
          legend.direction = "horizontal", 
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank()) + 
    guides(fill = guide_legend(ncol = 10)) 

brfss_plot <- poor_dist / fair_dist / good_dist / v_good_dist / excel_dist
    
wrap_elements(brfss_plot) + 
  ggtitle("Figure 2: Density plots of the proportion of different respones across states from 2002 to 2010")
```

<img src="p8105_hw3_ntw2117_files/figure-markdown_github/average proportion response category across location by state-1.png" width="100%" style="display: block; margin: auto;" />

Figure 2 shows density plots of the different response categories from 2002 to 2010. Because all of the plots are on the same scale, you can observe the average increase in the proportion of responses from poor up to very good. After very good, the proportion of responses then slightly decreases for the excellent category. It also appears that the distributions within each category remains relatively constant, likely indicating that response does not vary over time.

Problem 2
---------

#### Importing instacart data

``` r
insta_data <- p8105.datasets::instacart %>% 
  janitor::clean_names() %>% 
  mutate(order_dow = order_dow + 1)
```

#### Cleaning instacart

``` r
days <- c("Sunday",
          "Monday",
          "Tuesday",
          "Wednesday",
          "Thursday",
          "Friday",
          "Saturday")

insta_data$order_dow <- days[insta_data$order_dow]

insta_data$order_dow <- fct_relevel(insta_data$order_dow,
                                    "Sunday",
                                    "Monday",
                                    "Tuesday",
                                    "Wednesday",
                                    "Thursday",
                                    "Friday",
                                    "Saturday")
```

The Instacart dataset contains information on online grocery store orders during the year 2017 from the online company Instacart. The dataset has 1384617 observations and 15 variables. Each observation is a product from an order. Important variables include product IDs/names, the number of times the user has reordered a product, day and time information for variables, and how long it has been since a users last order. There are 131209 distinct customer orders.

#### Questions and plots

``` r
numb_orders <- insta_data %>%
  group_by(aisle) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
```

Fresh vegetables and fresh fruits are ordered most with 150609 and 150473 orders respectively. Followed by packaged vegetables and fruits. The least ordered items are those coming from the beauty aisle, 287 orders.

``` r
insta_data %>%
  group_by(aisle) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) %>% 
  mutate(aisle = forcats::fct_reorder(aisle, n, .desc = TRUE)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
    coord_flip() + 
    geom_bar(stat = "identity", fill = "#a888bd") +
    labs(y = "Number of Orders", 
         x = "Aisle") + 
    theme(panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank()) + 
    ggtitle("Figure 3: Number of orders across all aisles")
```

![](p8105_hw3_ntw2117_files/figure-markdown_github/items%20ordered%20in%20each%20aisle-1.png)

There are 134 aisles in the dataset. Figure 3 shows the number of orders across all aisles, ordered from least to most. While the plot is rather large, it is purposefully ordered such that aisles that are most ordered from are directly next to the x axis scale. In addition, by making a horizontal bar chart instead of vertical chart, the aisle names are more easily read than they would have been if they were angled on the x-axis.

``` r
insta_data %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarize(n = n()) %>% 
  filter(n == max(n)) %>% 
  arrange(desc(n)) %>%
  rename(Aisle = aisle, 
         "Product Name" = product_name, 
         "# Orders" = n) %>% 
  knitr::kable()
```

| Aisle                      | Product Name                                  |  \# Orders|
|:---------------------------|:----------------------------------------------|----------:|
| packaged vegetables fruits | Organic Baby Spinach                          |       9784|
| baking ingredients         | Light Brown Sugar                             |        499|
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |         30|

The above table shows the most popular items in the Baking Ingredients, Dog Food Care, and Packaged Vegetables and Fruits aisles; the observations are ordered by the number of orders. Organic baby spinach is the most frequently ordered item in the Packaged Vegetables and Fruits aisle with 9,784 orders. In the Baking ingredients aisle, the most popular item is Light Brown Sugar and in the Dog Food Care aisle the most popular item is Snack Sticks-Chickien and Rice Recipe Dog treats.

``` r
insta_data %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  select(product_name, order_dow, order_hour_of_day) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(avg_hour = round(mean(order_hour_of_day), 2)) %>%
  spread(key = order_dow, value = avg_hour) %>% 
  rename("Product Name" = product_name) %>% 
  knitr::kable(align = "c")
```

|   Product Name   | Sunday | Monday | Tuesday | Wednesday | Thursday | Friday | Saturday |
|:----------------:|:------:|:------:|:-------:|:---------:|:--------:|:------:|:--------:|
| Coffee Ice Cream |  13.77 |  14.32 |  15.38  |   15.32   |   15.22  |  12.26 |   13.83  |
| Pink Lady Apples |  13.44 |  11.36 |  11.70  |   14.25   |   11.55  |  12.78 |   11.94  |

This table contains the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered across every day of the week; the values are scaled for a 24 hour clock. Interestingly, Coffee Ice Cream is, on average, always ordered in the early afternoon for every day of the week while Pink Lady Apples range from being ordered, on average, after 11 am and after 2 pm.

Problem 3
---------

#### Importing and cleaning NY NOAA data

``` r
noaa_data <- p8105.datasets::ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, c("year", "month", "day"), sep = "-") %>% 
  mutate(tmax = as.double(tmax) / 10, 
         tmin = as.double(tmin) / 10, 
         prcp = prcp / 10,
         month = month.name[as.integer(month)], 
         year = as.integer(year))
```

The NOAA dataset contains weather information for all five New York state weather stations from 1981 to 2010. The dataset has 2595176 observations and 9 variables. Each observation represents one day from 1981 to 2010 for one of the weather stations the corresponding data was collected at. The variables provide information on that days minimum temperature and maximum temperature in degrees celsius, the snowfall for that day in millimeters, the snowdepth for that day in millimeters, and the amount of precipitation for that day in millimeters. It is important to note that uncleaned dataset reported minimum daily temperature and maximum daily temperature in tenths of degrees celsius and precipitation in tenths of millimeters. However, these were converted to proper units for ease of interpretation. The present NOAA dataset contains large amounts of missing data. Of all observations, 53% contain at least one missing value. In addition, 3% have missing values for all five key variables.

#### Questions and plots

The average amount of snowfall is 4.987 mm. However, the median amount of snowfall is 0 mm and the modal amount is 0 mm. The difference between the average amount of snowfall and the median indicates snowfall is right-skewed. This makes sense as most days out of a year there is no snowfall, but a couple of days will experience large amounts.

``` r
noaa_data %>% 
  filter(month %in% c("January", "July")) %>% 
  group_by(year, month) %>% 
  summarize(avg_temp = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = avg_temp)) + 
    geom_line(color = "#7496D2", size = 1) + 
  facet_grid(~ month, 
             scales = "free") + 
  labs(title = "Figure 4: Average maximum temperature (°C) in January and July, 1981-2010", 
       x = "Year", 
       y = "Average Daily Temperature (°C)") +
  theme(axis.text.x = element_text(angle = 45,
                                   vjust = 0.4), 
        strip.background = element_rect(fill = "black"), 
        strip.text = element_text(color = "white", 
                                  face = "bold"))
```

<img src="p8105_hw3_ntw2117_files/figure-markdown_github/average daily temp plot in january and july-1.png" width="100%" style="display: block; margin: auto;" />

Figure 4 shows the average maximum daily temperature in January and July from 1981-2010. The average max temperature in Jaunary tends to center around zero degrees while the temperature for July varies around 27 degrees. In addition, the average maximum temperature for January appears to have more variation compared to that of July.

``` r
temp_hex <- noaa_data %>%
  filter(!is.na(tmin),
         !is.na(tmax)) %>%
  ggplot(aes(x = tmin, y = tmax)) +
    geom_hex() + 
    viridis::scale_fill_viridis(name = "", 
                                option = "plasma") + 
    theme(legend.position = "bottom",  
          legend.key.height = unit(0.1, "in"), 
          legend.key.width = unit(0.5, "in"),
          plot.title = element_text(size = 7)) + 
    labs(title = "Hexagon density plot of daily minimum and maximum temperature (°C)",
         x = "Minimum Temperature (°C)", 
         y = "Maximum Temperature (°C)") + 
    scale_y_continuous(position = "right")
  
snow_density <- noaa_data %>%
  filter(snow > 0 & snow < 100) %>%
  mutate(year = as.factor(year)) %>%
  mutate(year_cat = fct_collapse(year, 
                                 "1981-85" = c("1981", "1982", "1983", "1984", "1985"), 
                                 "1986-90" = c("1986", "1987", "1988", "1989", "1990"), 
                                 "1991-95" = c("1991", "1992", "1993", "1994", "1995"), 
                                 "1996-00" = c("1996", "1997", "1998", "1999", "2000"), 
                                 "2001-05" = c("2001", "2002", "2003", "2004", "2005"), 
                                 "2006-10" = c("2006", "2007", "2008", "2009", "2010"))) %>% 
  ggplot(aes(x = snow, fill = year_cat, color = year_cat)) +
  geom_density(alpha = 0.01) +
    viridis::scale_fill_viridis(name = "Years", 
                                discrete = TRUE) + 
    viridis::scale_color_viridis(name = "Years",
                                 discrete = TRUE, 
                                 option = "viridis") + 
    labs(title = "Density plot of snowfall (mm), 1981-2010",
         x = "Snowfall (mm)", 
         y = "Density") + 
    theme(legend.position = "bottom", 
          legend.direction = "horizontal", 
          legend.key.size = unit(0.1, "in"), 
          legend.text = element_text(size = 6),
          plot.title = element_text(size = 7))

snow_temp_plot <- snow_density + temp_hex

wrap_elements(snow_temp_plot) + ggtitle("Figure 5: Density and hexagon plots")
```

<img src="p8105_hw3_ntw2117_files/figure-markdown_github/snow and temperature plot-1.png" width="100%" style="display: block; margin: auto;" />

Figure 5 shows a density plot of snowfall (mm) from 1981 to 2010, for observations with snowfall greater than zero and less than 100, and a hexagon density plot of the daily minimum and maximum temperature for all observations. For the density plot, while the question asked for the distribution of snowfall separetely by year I decided that collapsing the years from 30 levels to 6 produced a better graphic and displayed the same information in a clearer manner. It can be seen that there are distinct peaks in density around 20, 25, 50, and 75 mm. However, one can also see that the heights of these peaks has decreased over each 5 year period. This might be indicative of increasing temperatures as the result of climate change resulting in less snowfall.

In the hexagon density plot on the right, a relative linear relationship can be observed between minimum temperature and maximum temperature. It appears that the highest density of observations occurs in the middle of the plot and in this area there is an even more distinct positive, linear relationship. To speed up the processing time of the plot, I first filtered out missing values as these wouldn't be plotted either way.
